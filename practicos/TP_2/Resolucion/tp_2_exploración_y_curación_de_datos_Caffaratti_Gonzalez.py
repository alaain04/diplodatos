# -*- coding: utf-8 -*-
"""TP_2_Exploración_y_Curación_de_Datos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Abbg7pqtSdQNoYQ-UBnIcZfkf9J9IyPB

# **Mentoría**
## **Análisis y predicción de distribución troncal de energía**

### **Introducción**

En la siguiente notebook, se presentará la consigna a seguir para el segundo práctico de la materia Exploración y Curación de datos. El objetivo consiste en identificar e implementar los pasos necesarios para la limpieza y unificación de los datasets de Energía y Clima, así como también analizar cruces de datos con mayor profundidad y validando el sentido lógico. Para ello, comenzaremos con las importaciones pertinentes.

### Importaciones
"""

# Commented out IPython magic to ensure Python compatibility.
# Importación de las librerías necesarias
import numpy as np
import pandas as pd
# Puede que nos sirvan también
import matplotlib as mpl
mpl.get_cachedir()
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import datetime as dt
import warnings

pd.set_option('display.max_columns', 150)
pd.set_option('display.max_rows', 150)
pd.set_option('max_colwidth', 151)

"""## Consigna para Curación y Exploración del Dataset
### I. Rutina de Curación

Inicialmente, con el objetivo de preparar los datos que alimentarán futuros modelos de aprendizaje automático (ML), se propone seguir la siguiente [checklist](https://dimewiki.worldbank.org/wiki/Checklist:_Data_Cleaning) para la limpieza de los datos de nuestro proyecto. Esta checklist es la misma que utilizaron en el primer práctico de la materia y nos será de utilidad como guía para curar el dataset. A modo de ayuda, en esta notebook encontrarán una especie de template que sigue la checklist y que deberán ir completando.

Cada decisión tomada deberá quedar registrada de manera explícita y clara. Luego de pasar por todos los puntos de la checklist propuesta, deberán almacenar en un nuevo archivo los datos resultantes. 

A los fines de realizar este práctico, se utilizarán los dataset original, pero descartando todas aquellas columnas que se hayan calculado en base a features preexistentes, ya que éstas están relacionadas a decisiones que adoptaremos más adelante, como por ejemplo, sobre si es necesario crear nuevas features y si incluirlas o no. Recuerden que la ciencia de datos es un proceso circular, continuo y no lineal. Es decir, si los datos requieren de mayor procesamiento para satisfacer las necesidades de algoritmos de ML (cualesquiera de ellos), vamos a volver a la etapa inicial para, por ejemplo, crear nuevas features, tomar decisiones diferentes sobre valores faltantes o valores atípicos (outliers), descartar features, entre otras.

### II. Análisis en Profundidad del Contenido

Una vez aplicada la Checklist, lo que vamos a hacer es profundizar aún más el análisis y tomar decisiones que se consideren pertinentes, si es que no lo han hecho aún en el desarrollo del primer apartado. Por supuesto, se deberán registrar todas las decisiones que tomen al respecto.

Al finalizar con el práctico, las preguntas listadas a continuación deberán quedar respondidas, mientras que si ya lo hicieron durante el desarrollo de la ´checklist´, el objetivo es que se replanteen las decisiones tomadas al respecto:

1. La potencia total de las 3 fases esta dada por una fórmula en la que participan la tensión y corriente media de las fases. 

  1.a. Comparar los campos de tensiones. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? 

  1.b. Comparar los campos de corrientes. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? 
  
2.   Si la medición de potencia está invertida(negativa) el factor de potencia debería serlo también. Validar que esto ocurra en todos los casos. ¿Qué hacer en los que no?

3.   ¿Existe algun feture que pueda ser negativo? Reemplazar los negativos por su valor absoluto.

4.   ¿Existen valores faltantes? Definir e implementar estrategia para completarlos o descartarlos.

5.   Los datasets poseen diferentes frecuencia de medición. Definir una misma frecuencia y generar un único set de datos.


Esta lista es extensa e intenta abarcar todas las posibles irregularidades en los datos, pero puede no ser exhaustiva. Cualquier análisis adicional de consistencia que deseen agregar porque lo consideran pertinente, será bienvenido y valorado.

### Entregables

El entregable de este práctico consiste en esta misma Notebook, pero con la checklist realizada y el análisis de contenido completo, explicando las decisiones tomadas en cada etapa. Además, deberán generar un script (.py) que contenga una función para curar nuevos datos con la misma estructura. Finalmente, deberán actualizar la metadata.

# Resolución
## I. Rutina de Curación
### 1. Importación de Datos

**1.1. Verificación de inexistencia en problemas en la importación de los datasets**

**1.2. Asegurar la existencia de IDs o claves únicas**

El siguiente paso implica chequear que no existen datos duplicados y que las claves, si existen, son únicas.

**1.3. Despersonalizar datos y guardarlos en un nuevo archivo**

Aplica?

**1.4. Nunca modificar los dDatos crudos u originales**

Al finalizar la limpieza, deberán guardar el dataset resultante, para asegurarse de no modificar los datos originales.


### 2. Pasos de Limpieza Necesarios
**2.1. Etiquetas de variables y problemas de codificación/encoding**

Verificar que las etiquetas y valores de features no posean problemas de codificación. 

**2.2. Tratamiento de valores faltantes**

Para analizar los valores faltantes, primero deberán saber cuántos existen por campo y cuánto representan del total. 

Detallar estrategias para completar los features faltantes o eliminar el registro completo.

**2.3. Codificación de variables categóricas**

En el caso de crear variables categóricas, como nombre dias, meses, o booleanos de tipo "si/no" se deberán codificar en valores numéricos para incluirlos en los datos de entrada de los modelos.

**2.4. No cambiar los nombres de las variables de la fuente de origen**


**2.5. Verificación de consistencia de datos**

Este es el paso más analítico, en donde se deben aplicar reglas de integridad.


**2.6. Identificar y documentar valores atípicos/outliers**

Calcular estadísticos. 

Detallar estrategias para completar los features con outliers o eliminar el registro completo.


**2.7. Evaluar cómo comprimir los datos para su almacenamiento más eficiente**

**2.8. Guardar el set de datos con un nombre informativo**


### 3. Pasos de Limpieza Deseables
**3.1. Ordenar variables/columnas**


**3.2. Quitar variables/columnas irrelevantes**

Existen features irrelevantes, que no aportan información?

**3.3. Agregar metadata a los datos**

Cuando y como fueron obtenidos, limpieza realizada, decisiones implementadas, asunciones, etc.

### II. Análisis en Profundidad del Contenido

1. La potencia total de las 3 fases esta dada por una fórmula en la que participan la tensión y corriente media de las fases. 

  1.a. Comparar los campos de tensiones. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? 

  1.b. Comparar los campos de corrientes. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? 
  1.c Valor absoluto de la potencia
2.   Si la medición de potencia está invertida(negativa) el factor de potencia debería serlo también. Validar que esto ocurra en todos los casos. ¿Qué hacer en los que no?

3.   ¿Existe algun feture que pueda ser negativo? Reemplazar los negativos por su valor absoluto.

4.   ¿Existen valores faltantes? Definir e implementar estrategia para completarlos o descartarlos.

5.   Los datasets poseen diferentes frecuencia de medición. Definir una misma frecuencia y generar un único set de datos.
"""



"""## I. Rutina de Curación
### 1. Importación de Datos

**1.1. Verificación de inexistencia en problemas en la importación de los datasets**

*No se encontraron problemas al abrir archivo .csv desde origen de datos con codificación default 'utf-8'*

**1.2. Asegurar la existencia de IDs o claves únicas**

El siguiente paso implica chequear que no existen datos duplicados y que las claves, si existen, son únicas.

*No existe un campo ID único y se presentan valores duplicados para el campo de fecha-hora.  Se podría generar algun tipo de campo hash combinando algunos campos*

**1.3. Despersonalizar datos y guardarlos en un nuevo archivo**

Aplica?

*No aplicaría la despersonalización o encriptación, ya que no existen datos sensibles de personas o normativas vigentes sobre los datos recoletados de los sensores de la linea eléctrica.*

**1.4. Nunca modificar los dDatos crudos u originales**

Al finalizar la limpieza, deberán guardar el dataset resultante, para asegurarse de no modificar los datos originales.

*Se guardaría el resultado de limpieza utilizando la funcion de pandas "to_csv" o si el archivo es muy grande incluir la opcion de compresión.*

### 2. Pasos de Limpieza Necesarios
**2.1. Etiquetas de variables y problemas de codificación/encoding**

Verificar que las etiquetas y valores de features no posean problemas de codificación. 

*Los nombre de los campos no poseen caracteres extraños. Los datos del dataset son numericos o fechas. Se pueden incluir nuevas columnas calculadas con datos categóricos basados en el tipo de datos FECHA-HORA pero se los puede pre-procesar para convertirlos en valor numérico o binarizarlos*

**2.2. Tratamiento de valores faltantes**

Para analizar los valores faltantes, primero deberán saber cuántos existen por campo y cuánto representan del total. 

Detallar estrategias para completar los features faltantes o eliminar el registro completo.
"""

#Parsing auxiliar
dateparse = lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

_lista = []
def log_metadata(texto):
    _lista.append({
        'fecha_hora': dt.datetime.now().strftime('%Y%m%d-%H%M%S'),
        'texto': texto
    })

_ds_energia = pd.read_csv('https://raw.githubusercontent.com/alaain04/diplodatos/master/data/energia_completo.csv',sep=';')

log_metadata('Dataset leído correctamente en utf-8')

_ds_energia['hora'] = _ds_energia.Fecha.apply(lambda x: x[11:13])
_ds_energia['Fecha'] = pd.to_datetime(_ds_energia['Fecha'],format='%Y-%m-%d %H:%M:%S')
_ds_energia.head(5)

#Elimino datos del día 3/12/2019 para comenzar un dia completo
_ds_energia.drop(_ds_energia[pd.to_datetime(_ds_energia['Fecha'].dt.date)=='2019-12-03'].index,inplace=True)

#Generamos Period Index y ordenamos el dataset de Energia
_ds_energia.index = pd.PeriodIndex(list(_ds_energia['Fecha']), freq='05T')
_ds_energia = _ds_energia.sort_index()

log_metadata('Generamos Period Index para dataset de Energía y ordenamos.')

# _ds_clima
_ds_clima = pd.read_csv('https://raw.githubusercontent.com/alaain04/diplodatos/master/data/clima_posadas_20192020.csv')
_ds_clima['time'] = pd.to_datetime(_ds_clima['time'],format='%Y-%m-%d %H:%M:%S')

#Elegimos features del dataset de Clima y los llevamos cada 5 minutos para poder unirlo con el dataset de energia
_ds_clima.index = pd.PeriodIndex(list(_ds_clima['time']), freq='T')
_ds_clima = _ds_clima[['temperature','windspeed','winddirection']].resample('05T').fillna("backfill")
#Ordenamos valores
_ds_clima = _ds_clima.sort_index()

log_metadata('Generamos Period Index para dataset de Clima y ordenamos valores')

"""#####Unimos ambos datasets"""

_ds_energia=_ds_energia.join(_ds_clima,how='left')

log_metadata('Unimos ambos datasets')

"""#####Creamos campos para ayudar en los cálculos"""

#Calculamos los valores absolutos de la Potencia
_ds_energia['abs_Kwatts'] = _ds_energia['Kwatts_3_fases'].abs()

#Calculamos los valores absolutos de la Potencia
_ds_energia['abs_Potencia'] = _ds_energia['Factor_de_Poten_A'].abs()

#Creamos un campo con la fecha del día solamente
_ds_energia['fecha_dia'] = pd.to_datetime(_ds_energia['Fecha'].dt.date)

"""#####Calculo de campos NAN en las variables importantes"""

#print('La Cantidad de outliers en Potencia es: '+str(len(_ds_energia[_ds_energia.outlier_Kwatts].abs_Kwatts)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.outlier_Kwatts].abs_Kwatts)/len(_ds_energia.abs_Kwatts)*100) +"%.")
print('La Cantidad de Nan en Potencia es: '+str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)/len(_ds_energia.abs_Kwatts)*100) +"%.")
print('La Cantidad de Nan en las tensiones Vab es: '+str(len(_ds_energia[_ds_energia.Vab.isna()].Vab)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vab.isna()].Vab)/len(_ds_energia.abs_Kwatts)*100) +"%.")
print('La Cantidad de Nan en las tensiones Vbc es: '+str(len(_ds_energia[_ds_energia.Vbc.isna()].Vbc)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vab.isna()].Vbc)/len(_ds_energia.abs_Kwatts)*100) +"%.")
print('La Cantidad de Nan en las tensiones Vca es: '+str(len(_ds_energia[_ds_energia.Vca.isna()].Vca)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vca.isna()].Vca)/len(_ds_energia.abs_Kwatts)*100) +"%.")
print()
print('La Cantidad de Nan en Ds_Clima.temperature es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.temperature.count()))
print('La Cantidad de Nan en Ds_Clima.windspeed es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.windspeed.count()))
print('La Cantidad de Nan en Ds_Clima.winddirection es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.winddirection.count()))

log_metadata('Calculamos Nan')
log_metadata('La Cantidad de Nan en Potencia es: '+str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)/len(_ds_energia.abs_Kwatts)*100) +"%.")
log_metadata('La Cantidad de Nan en las tensiones Vab es: '+str(len(_ds_energia[_ds_energia.Vab.isna()].Vab)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vab.isna()].Vab)/len(_ds_energia.abs_Kwatts)*100) +"%.")
log_metadata('La Cantidad de Nan en las tensiones Vbc es: '+str(len(_ds_energia[_ds_energia.Vbc.isna()].Vbc)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vab.isna()].Vbc)/len(_ds_energia.abs_Kwatts)*100) +"%.")
log_metadata('La Cantidad de Nan en las tensiones Vca es: '+str(len(_ds_energia[_ds_energia.Vca.isna()].Vca)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vca.isna()].Vca)/len(_ds_energia.abs_Kwatts)*100) +"%.")

log_metadata('La Cantidad de Nan en Ds_Clima.temperature es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.temperature.count()))
log_metadata('La Cantidad de Nan en Ds_Clima.windspeed es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.windspeed.count()))
log_metadata('La Cantidad de Nan en Ds_Clima.winddirection es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.winddirection.count()))



"""**2.3. Codificación de variables categóricas**

Aplica?

*En el caso de crear variables categóricas, como nombre días, meses, o booleanos de tipo "si/no" se deberán codificar en valores numéricos para incluirlos en los datos de entrada de los modelos*

######Se crea variable de nombre de dia y mes según fecha
"""

_ds_energia['DiaSemana'] = pd.to_datetime(_ds_energia.fecha_dia.dt.date).dt.day_name()
_ds_energia['mes_desc'] = pd.to_datetime(_ds_energia.fecha_dia.dt.date).dt.month_name()

"""#####Funcion para definir si es dia laboral o no"""

def get_dia_laboral(nombre_dia):
    if nombre_dia in ['Wednesday', 'Thursday', 'Friday', 'Monday','Tuesday']:
        return 'Dia laboral'
    else:
        return 'Fin de semana'

_ds_energia['es_dia_laboral'] = _ds_energia['DiaSemana'].apply(lambda x:get_dia_laboral(x))

"""####Codificación de variables

Se utilizará la clase labelEncoder para no generar columnas adicionales al set de datos.
"""

# importamos clase de preprocessing de la librería sklearn
from sklearn import preprocessing

# instanciamos clases
le_dia_semana = preprocessing.LabelEncoder()
le_dia_laboral = preprocessing.LabelEncoder()

# Ejecutamos la funcion entrena el modelo de codificación
le_dia_semana.fit(_ds_energia['DiaSemana'])
le_dia_laboral.fit(_ds_energia['es_dia_laboral'])

"""Visualizamos la transformacion de los datos"""

# View encoder mapping
dict(zip(le_dia_semana.classes_,le_dia_semana.transform(le_dia_semana.classes_)))

# View encoder mapping
dict(zip(le_dia_laboral.classes_,le_dia_laboral.transform(le_dia_laboral.classes_)))

"""Incorporamos las variables codificadas a números en el set de datos"""

# transfomr -> ejecuta el modelo y retorna el array con los datos transformados

_ds_energia['DiaSemana_Transform'] = le_dia_semana.transform(_ds_energia['DiaSemana']) 
_ds_energia['Es_dia_laboral_Transform'] = le_dia_laboral.transform(_ds_energia['es_dia_laboral']) 
_ds_energia.sample(3)

"""**2.4. No cambiar los nombres de las variables de la fuente de origen**

*No se modifican los nombre de campos del dataset*

####**Reglas de integridad**

**2.5. Verificación de consistencia de datos**

Este es el paso más analítico, en donde se deben aplicar reglas de integridad.

*Se verificará la integridad de los campos corriente, tensión y potencia. Estos valores no deben ser negativos. En el caso de potencia y factor de potencia, los valores negativos se pasan a valor absoluto.*
"""

#Verificar si existen valores negativos
print('Cant. negativos Amper fase T-A: ' + str(_ds_energia[_ds_energia['Amper_fase_T_A'] < 0].shape[0] ))
print('Cant. negativos Amper fase S-A: ' +  str(_ds_energia[_ds_energia['Amper_fase_S_A'] < 0].shape[0] ))
print('Cant. negativos Amper fase R-A: ' +  str(_ds_energia[_ds_energia['Amper_fase_R_A'] < 0].shape[0] ))

print('Cant. negativos Vab: ' +  str(_ds_energia[_ds_energia['Vab'] < 0].shape[0] ))
print('Cant. negativos Vca: ' +  str(_ds_energia[_ds_energia['Vca'] < 0].shape[0] ))
print('Cant. negativos Vbc: ' +  str(_ds_energia[_ds_energia['Vbc'] < 0].shape[0] ))

print('Cant. negativos Kwatts 3 fases: ' +  str(_ds_energia[_ds_energia['Kwatts_3_fases'] < 0].shape[0] ))
print('Cant. negativos Factor de Poten-A: ' +  str(_ds_energia[_ds_energia['Factor_de_Poten_A'] < 0].shape[0] ))

log_metadata('Cant. negativos Amper fase T-A: ' + str(_ds_energia[_ds_energia['Amper_fase_T_A'] < 0].shape[0] ))
log_metadata('Cant. negativos Amper fase S-A: ' +  str(_ds_energia[_ds_energia['Amper_fase_S_A'] < 0].shape[0] ))
log_metadata('Cant. negativos Amper fase R-A: ' +  str(_ds_energia[_ds_energia['Amper_fase_R_A'] < 0].shape[0] ))

log_metadata('Cant. negativos Vab: ' +  str(_ds_energia[_ds_energia['Vab'] < 0].shape[0] ))
log_metadata('Cant. negativos Vca: ' +  str(_ds_energia[_ds_energia['Vca'] < 0].shape[0] ))
log_metadata('Cant. negativos Vbc: ' +  str(_ds_energia[_ds_energia['Vbc'] < 0].shape[0] ))

log_metadata('Cant. negativos Kwatts 3 fases: ' +  str(_ds_energia[_ds_energia['Kwatts_3_fases'] < 0].shape[0] ))
log_metadata('Cant. negativos Factor de Poten-A: ' +  str(_ds_energia[_ds_energia['Factor_de_Poten_A'] < 0].shape[0] ))

"""Control de registros duplicados en fecha_hora"""

print("Cant. registros de fecha-hora duplicados: " + str(_ds_energia[_ds_energia.Fecha.duplicated()].shape[0]))

log_metadata("Cant. registros de fecha-hora duplicados: " + str(_ds_energia[_ds_energia.Fecha.duplicated()].shape[0]))

"""##### **Control Outliers**
**2.6. Identificar y documentar valores atípicos/outliers**

Calcular estadísticos. 

Detallar estrategias para completar los features con outliers o eliminar el registro completo.
"""

_ds_energia.boxplot(column='abs_Kwatts')

"""Control de valores fuera de rango (outliers)
Como existen valores outliers muy extremos la "media" no es un valor que sea representativo, por lo que se utliza la  "mediana". Si tomamos 3 veces el valor de la mediana como valor máximo posible, todos los registros con Consumo por encima de este valor serán considerados outliers
"""

# obtengo lista de registros outliers
outl = _ds_energia[_ds_energia.abs_Kwatts > (_ds_energia.abs_Kwatts.median() + 3 * _ds_energia.abs_Kwatts.median())] 
outl.shape

"""Sin outliers, el gráfico queda más entendible"""

_ds_energia.drop(outl.index).boxplot(column='abs_Kwatts');

"""#####Estrategia de para manejar outliers

Consideramos los valores outliers no solo como error de medición de instrumentos sino como cortes efectivos de energía. Se lo identificarán con valor 1 en una columna nueva, antes de reemplazar su valor por cero (0) ya que cero es un indicador de corte de energía
"""

_valor_outlier = _ds_energia.abs_Kwatts.median() + 3 * _ds_energia.abs_Kwatts.median()
print('Límite máximo de consumo para considerar outliers: ' + str(_valor_outlier) )

log_metadata('Límite máximo de consumo para considerar outliers es ' + str(_valor_outlier) +'. Tenemos ' + str(outl.abs_Kwatts.count()) + ' cantidad de outliers en la potencia.' )

#Reemplazamos Nan en tensiones
_ds_energia[ _ds_energia['Vab'].isna()] = 0
_ds_energia[ _ds_energia['Vca'].isna()] = 0
_ds_energia[ _ds_energia['Vbc'].isna()] = 0

# convertimos los nan de abs_Kwatts en valor 0 si es nan 'Kwatts 3 fases' y 'Amper fase T-A'== 0, luego se marcará como un corte de energia
_ds_energia.loc[ ( _ds_energia['Amper_fase_R_A'].isna()) & (_ds_energia['Amper_fase_T_A'] == 0), 'Amper_fase_R_A'] = 0
_ds_energia.loc[ ( _ds_energia['Amper_fase_S_A'].isna()) & (_ds_energia['Amper_fase_T_A'] == 0), 'Amper_fase_S_A'] = 0
_ds_energia.loc[ ( _ds_energia['Kwatts_3_fases'].isna()) & (_ds_energia['Amper_fase_T_A'] == 0), 'abs_Kwatts'] = 0

#Evaluamos si hubo un corte de energia (o sea, si la potencia total es igual a 0)
_ds_energia.loc[_ds_energia.abs_Kwatts == 0, 'corte_energia'] = 1
_ds_energia.loc[(_ds_energia['Factor_de_Poten_A']<=0) & (_ds_energia['Kwatts_3_fases']>0),
                                               'corte_energia'] = 1
_ds_energia.loc[(_ds_energia['Factor_de_Poten_A']>0) & (_ds_energia['Kwatts_3_fases']<0),
                                               'corte_energia'] = 1                                               

_ds_energia.loc[_ds_energia.abs_Kwatts != 0, 'corte_energia'] = 0


#Cambiamos outliers sólo en columna nueva
#Evaluamos si hay los outliers de la potencia
_ds_energia.loc[_ds_energia.abs_Kwatts > _valor_outlier, 'outlier_Kwatts'] = 1
_ds_energia.loc[_ds_energia.abs_Kwatts <= _valor_outlier, 'outlier_Kwatts'] = 0


#Decidimos reemplazar los valores outliers de Potencia por 0 ya que consideramos que fue un error de medición y que para poder graficar los datos, necesitamos que no estén.
_ds_energia.loc[_ds_energia['outlier_Kwatts']==1, 'abs_Kwatts'] = 0

_ds_energia.winddirection.fillna(value=0,inplace=True)
_ds_energia.windspeed.fillna(value=0,inplace=True)

print('La Cantidad de Nan en Potencia es: '+str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)/len(_ds_energia.abs_Kwatts)*100) +"%.")

log_metadata('Calculamos Nan luego de reemplazarl')
log_metadata('La Cantidad de Nan en Potencia es: '+str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.abs_Kwatts.isna()].abs_Kwatts)/len(_ds_energia.abs_Kwatts)*100) +"%.")
log_metadata('La Cantidad de Nan en las tensiones Vab es: '+str(len(_ds_energia[_ds_energia.Vab.isna()].Vab)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vab.isna()].Vab)/len(_ds_energia.abs_Kwatts)*100) +"%.")
log_metadata('La Cantidad de Nan en las tensiones Vbc es: '+str(len(_ds_energia[_ds_energia.Vbc.isna()].Vbc)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vab.isna()].Vbc)/len(_ds_energia.abs_Kwatts)*100) +"%.")
log_metadata('La Cantidad de Nan en las tensiones Vca es: '+str(len(_ds_energia[_ds_energia.Vca.isna()].Vca)) + ", con un impacto de un " + str(len(_ds_energia[_ds_energia.Vca.isna()].Vca)/len(_ds_energia.abs_Kwatts)*100) +"%.")

log_metadata('La Cantidad de Nan en Ds_Clima.temperature es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.temperature.count()))
log_metadata('La Cantidad de Nan en Ds_Clima.windspeed es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.windspeed.count()))
log_metadata('La Cantidad de Nan en Ds_Clima.winddirection es: '+str(_ds_energia.fecha_dia.count() - _ds_energia.winddirection.count()))

"""####Almacenamiento de resultado
**2.7. Evaluar cómo comprimir los datos para su almacenamiento más eficiente**

Si el set de datos es muy grande (tomar un valor referencia, ejemplo 10 mb) se puede comprimir el mismo en .zip

Utilizar una nomenclatura de nombre de archivo para que con cada ejecucion se puesa saber ráápidamente cual fue el archivo generado en el directorio de trabajo

Ejemplo:

../output/clean_data/ds_energia_output_yyyymmdd_hhmmss.zip
"""

import os
import errno
#path para el archivo de salida
_path_dir = os.path.join(os.getcwd(),'output/clean_data')

# verificar si directorio existe
try:
    # crear directorio si no existe
    os.makedirs(_path_dir)
except OSError as exception:
    if exception.errno != errno.EEXIST:
        raise

"""**2.8. Guardar el set de datos con un nombre informativo**"""

# crear máscara de archivo
_filename = 'ds_energia_output_{}_{}'.format(dt.datetime.now().strftime('%y%m%d'),dt.datetime.now().strftime('%H%M%S'))
_extension = '.csv'
_filename_extension = _filename + _extension
_extension_zip = '.zip'
_filename_zip_extension = _filename + _extension_zip

try:
  #Exportamos el csv sin las columnas originales de Kwatts 3 fases y Factor de potencia
    _ds_energia.drop(columns=['Kwatts_3_fases','Factor_de_Poten_A']).to_csv(os.path.join(_path_dir,_filename_extension))
except Exception as e:
    print(str(e))
    
log_metadata('Se crea un archivo csv :' + str(os.path.join(_path_dir,_filename_extension)))

# generar zip en directorio
import zipfile
with zipfile.ZipFile(os.path.join(_path_dir,_filename_zip_extension),'w',zipfile.ZIP_DEFLATED) as _zipedfile: 
    _zipedfile.write(os.path.join(_path_dir,_filename_extension),_filename_extension)

log_metadata('Zipeamos el archivo ')

_filename_metadata = 'ds_energia_metadata_{}_{}.txt'.format(dt.datetime.now().strftime('%y%m%d'),dt.datetime.now().strftime('%H%M%S'))

# Generamos archivo metadata

_f = open(os.path.join(_path_dir,_filename_metadata), 'w')

# cabecera archivo
_f.write(f'Fecha-Hora\tMensaje\r\n')

for item in _lista:
    print(item['fecha_hora'] , item['texto'])
    
    # archivo separado por tab
    _f.write('{}\t{}\r\n'.format(item['fecha_hora'],item['texto']))

_f.close()

"""### II. Análisis en Profundidad del Contenido

1. La potencia total de las 3 fases esta dada por una fórmula en la que participan la tensión y corriente media de las fases. 
**Potencia = Tension x Corriente x Coseno de Phi (esto en el caso de las potencia activa)**

[Distintos tipos de potencia](http://www.asifunciona.com/electrotecnia/ke_factor_potencia/ke_factor_potencia_3.htm)

  1.a. Comparar los campos de tensiones. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? 
  **Los campos de tensiones no poseen la misma información ya que cada uno tiene la tensión de cada fase. Si bien son datos similares, no podemos unificarlos ya que son necesarios si queremos analizar la información de cada fase.**
"""

_ds_energia[['Vab','Vca','Vbc']].describe()

"""1.b. Comparar los campos de corrientes. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? 
  **Los 3 campos de corrientes proveen un amperaje según los tipos de potencias: activa, reactiva y aparente. Los datos no son los mismos y no puede tomarse 1 como general porque cada variación tiene que ver son el comportamiento de la corriente de cada fase.**
"""

_ds_energia[['Amper_fase_T_A', 'Amper_fase_S_A', 'Amper_fase_R_A']].describe()

"""1.c Valor absoluto de la potencia"""

_ds_energia[['abs_Kwatts']].describe()

"""2.   Si la medición de potencia está invertida(negativa) el factor de potencia debería serlo también. Validar que esto ocurra en todos los casos. ¿Qué hacer en los que no?"""

_ds_energia[(_ds_energia['Factor_de_Poten_A']<=0) & (_ds_energia['Kwatts_3_fases']>0)].count()

_ds_energia[(_ds_energia['Factor_de_Poten_A']>=0) & (_ds_energia['Kwatts_3_fases']<0)].count()

"""**Se considera que el factor de potencia es negativo y el consumo en Kw es positivo o viserversa como un corte de energía. Se tomarán sus valores absolutos para realizar cálculos.**

3.   ¿Existe algun feature que pueda ser negativo? Reemplazar los negativos por su valor absoluto.

**El factor de Potencia y la potencia total tenían valores negativos que fueron pasados a su valor absoluto.**

4.   ¿Existen valores faltantes? Definir e implementar estrategia para completarlos o descartarlos.

**En el caso del dataset de clima, al llevarlo a una frecuencia de 5 mintos (cuando el dataset original viene cada una hora), aplicamos un backfill llenando con valores faltantes con datos de la hora siguiente.**

5.   Los datasets poseen diferentes frecuencia de medición. Definir una misma frecuencia y generar un único set de datos.

**Definimos una frecuencia de 5 minutos en el dataset de clima para que coincidan con los datos del dataset de energía y no perder detalles de la información del servicio eléctrico. La unión está hecha al comenzar el ETL.**
"""