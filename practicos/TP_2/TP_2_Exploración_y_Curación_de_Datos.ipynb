{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP 2 - Exploración y Curación de Datos.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my4cfXmQxPgD",
        "colab_type": "text"
      },
      "source": [
        "# **Mentoría**\n",
        "## **Análisis y predicción de distribución troncal de energía**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BfUSjeJasF5",
        "colab_type": "text"
      },
      "source": [
        "### **Introducción**\n",
        "\n",
        "En la siguiente notebook, se presentará la consigna a seguir para el segundo práctico de la materia Exploración y Curación de datos. El objetivo consiste en identificar e implementar los pasos necesarios para la limpieza y unificación de los datasets de Energía y Clima, así como también analizar cruces de datos con mayor profundidad y validando el sentido lógico. Para ello, comenzaremos con las importaciones pertinentes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg4kXMZGau4Y",
        "colab_type": "text"
      },
      "source": [
        "### Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvoEMroCakp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importación de las librerías necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Puede que nos sirvan también\n",
        "import matplotlib as mpl\n",
        "mpl.get_cachedir()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "import warnings"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZHaBnlJa6i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_columns', 150)\n",
        "pd.set_option('display.max_rows', 150)\n",
        "pd.set_option('max_colwidth', 151)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmetljyza4ZI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Consigna para Curación y Exploración del Dataset\n",
        "### I. Rutina de Curación\n",
        "\n",
        "Inicialmente, con el objetivo de preparar los datos que alimentarán futuros modelos de aprendizaje automático (ML), se propone seguir la siguiente [checklist](https://dimewiki.worldbank.org/wiki/Checklist:_Data_Cleaning) para la limpieza de los datos de nuestro proyecto. Esta checklist es la misma que utilizaron en el primer práctico de la materia y nos será de utilidad como guía para curar el dataset. A modo de ayuda, en esta notebook encontrarán una especie de template que sigue la checklist y que deberán ir completando.\n",
        "\n",
        "Cada decisión tomada deberá quedar registrada de manera explícita y clara. Luego de pasar por todos los puntos de la checklist propuesta, deberán almacenar en un nuevo archivo los datos resultantes. \n",
        "\n",
        "A los fines de realizar este práctico, se utilizarán los dataset original, pero descartando todas aquellas columnas que se hayan calculado en base a features preexistentes, ya que éstas están relacionadas a decisiones que adoptaremos más adelante, como por ejemplo, sobre si es necesario crear nuevas features y si incluirlas o no. Recuerden que la ciencia de datos es un proceso circular, continuo y no lineal. Es decir, si los datos requieren de mayor procesamiento para satisfacer las necesidades de algoritmos de ML (cualesquiera de ellos), vamos a volver a la etapa inicial para, por ejemplo, crear nuevas features, tomar decisiones diferentes sobre valores faltantes o valores atípicos (outliers), descartar features, entre otras.\n",
        "\n",
        "### II. Análisis en Profundidad del Contenido\n",
        "\n",
        "Una vez aplicada la Checklist, lo que vamos a hacer es profundizar aún más el análisis y tomar decisiones que se consideren pertinentes, si es que no lo han hecho aún en el desarrollo del primer apartado. Por supuesto, se deberán registrar todas las decisiones que tomen al respecto.\n",
        "\n",
        "Al finalizar con el práctico, las preguntas listadas a continuación deberán quedar respondidas, mientras que si ya lo hicieron durante el desarrollo de la ´checklist´, el objetivo es que se replanteen las decisiones tomadas al respecto:\n",
        "\n",
        "1. La potencia total de las 3 fases esta dada por una fórmula en la que participan la tensión y corriente media de las fases. \n",
        "\n",
        "  1.a. Comparar los campos de tensiones. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? \n",
        "\n",
        "  1.b. Comparar los campos de corrientes. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? \n",
        "  \n",
        "2.   Si la medición de potencia está invertida(negativa) el factor de potencia debería serlo también. Validar que esto ocurra en todos los casos. ¿Qué hacer en los que no?\n",
        "\n",
        "3.   ¿Existe algun feture que pueda ser negativo? Reemplazar los negativos por su valor absoluto.\n",
        "\n",
        "4.   ¿Existen valores faltantes? Definir e implementar estrategia para completarlos o descartarlos.\n",
        "\n",
        "5.   Los datasets poseen diferentes frecuencia de medición. Definir una misma frecuencia y generar un único set de datos.\n",
        "\n",
        "\n",
        "Esta lista es extensa e intenta abarcar todas las posibles irregularidades en los datos, pero puede no ser exhaustiva. Cualquier análisis adicional de consistencia que deseen agregar porque lo consideran pertinente, será bienvenido y valorado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmMltLhGgrKy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Entregables\n",
        "\n",
        "El entregable de este práctico consiste en esta misma Notebook, pero con la checklist realizada y el análisis de contenido completo, explicando las decisiones tomadas en cada etapa. Además, deberán generar un script (.py) que contenga una función para curar nuevos datos con la misma estructura. Finalmente, deberán actualizar la metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMxI9mTvhM26",
        "colab_type": "text"
      },
      "source": [
        "# Resolución\n",
        "## I. Rutina de Curación\n",
        "### 1. Importación de Datos\n",
        "**1.1. Verificación de Inexistencia de Problemas en la Importación**\n",
        "\n",
        "Para comenzar, importamos los datos que vamos a procesar:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8J7Ystqhd8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parsing auxiliar\n",
        "dateparse = lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BkLmzWThfnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# _ds_energía\n",
        "_ds_energia = pd.read_csv('https://raw.githubusercontent.com/alaain04/diplodatos/master/data/distribucion.csv',\n",
        "                          dtype={'Amper fase T-A': float},\n",
        "                          parse_dates=['Fecha'],\n",
        "                          date_parser=dateparse,\n",
        "                          float_precision='round_trip')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mB-dKa6h-mt",
        "colab_type": "text"
      },
      "source": [
        "Recuerden que la variable Target constituye nuestro objetivo de predicción. Es la etiqueta de los datos de acuerdo al nivel de pobreza del hogar que habitan, según la siguiente escala o clases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeDJq-b6iDyz",
        "colab_type": "text"
      },
      "source": [
        "# Resolución\n",
        "## I. Rutina de Curación\n",
        "### 1. Importación de Datos\n",
        "\n",
        "**1.1. Verificación de inexistencia en problemas en la importación de los datasets**\n",
        "\n",
        "**1.2. Asegurar la existencia de IDs o claves únicas**\n",
        "\n",
        "El siguiente paso implica chequear que no existen datos duplicados y que las claves, si existen, son únicas.\n",
        "\n",
        "**1.3. Despersonalizar datos y guardarlos en un nuevo archivo**\n",
        "\n",
        "Aplica?\n",
        "\n",
        "**1.4. Nunca modificar los dDatos crudos u originales**\n",
        "\n",
        "Al finalizar la limpieza, deberán guardar el dataset resultante, para asegurarse de no modificar los datos originales.\n",
        "\n",
        "\n",
        "### 2. Pasos de Limpieza Necesarios\n",
        "**2.1. Etiquetas de variables y problemas de codificación/encoding**\n",
        "\n",
        "Verificar que las etiquetas y valores de features no posean problemas de codificación. \n",
        "\n",
        "**2.2. Tratamiento de valores faltantes**\n",
        "\n",
        "Para analizar los valores faltantes, primero deberán saber cuántos existen por campo y cuánto representan del total. \n",
        "\n",
        "Detallar estrategias para completar los features faltantes o eliminar el registro completo.\n",
        "\n",
        "**2.3. Codificación de variables categóricas**\n",
        "\n",
        "Aplica?\n",
        "\n",
        "**2.4. No cambiar los nombres de las variables de la fuente de origen**\n",
        "\n",
        "\n",
        "**2.5. Verificación de consistencia de datos**\n",
        "\n",
        "Este es el paso más analítico, en donde se deben aplicar reglas de integridad.\n",
        "\n",
        "\n",
        "**2.6. Identificar y documentar valores atípicos/outliers**\n",
        "\n",
        "Calcular estadísticos. \n",
        "\n",
        "Detallar estrategias para completar los features con outliers o eliminar el registro completo.\n",
        "\n",
        "\n",
        "**2.7. Evaluar cómo comprimir los datos para su almacenamiento más eficiente**\n",
        "\n",
        "**2.8. Guardar el set de datos con un nombre informativo**\n",
        "\n",
        "\n",
        "### 3. Pasos de Limpieza Deseables\n",
        "**3.1. Ordenar variables/columnas**\n",
        "\n",
        "\n",
        "**3.2. Quitar variables/columnas irrelevantes**\n",
        "\n",
        "Existen features irrelevantes, que no aportan información?\n",
        "\n",
        "**3.3. Agregar metadata a los datos**\n",
        "\n",
        "Cuando y como fueron obtenidos, limpieza realizada, decisiones implementadas, asunciones, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKVtEcsMycT8",
        "colab_type": "text"
      },
      "source": [
        "### II. Análisis en Profundidad del Contenido\n",
        "\n",
        "1. La potencia total de las 3 fases esta dada por una fórmula en la que participan la tensión y corriente media de las fases. \n",
        "\n",
        "  1.a. Comparar los campos de tensiones. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? \n",
        "\n",
        "  1.b. Comparar los campos de corrientes. ¿Poseen la misma información? ¿Qué deberíamos hacer al respecto? \n",
        "  \n",
        "2.   Si la medición de potencia está invertida(negativa) el factor de potencia debería serlo también. Validar que esto ocurra en todos los casos. ¿Qué hacer en los que no?\n",
        "\n",
        "3.   ¿Existe algun feture que pueda ser negativo? Reemplazar los negativos por su valor absoluto.\n",
        "\n",
        "4.   ¿Existen valores faltantes? Definir e implementar estrategia para completarlos o descartarlos.\n",
        "\n",
        "5.   Los datasets poseen diferentes frecuencia de medición. Definir una misma frecuencia y generar un único set de datos."
      ]
    }
  ]
}