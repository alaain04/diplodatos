{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mentoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduccion al Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente notebook se presentará la consigna a seguir para el tercer práctico del proyecto, correspondiente a la materia Introducción al Aprendizaje Automático. El objetivo consiste en explorar la aplicación de diferentes métodos de aprendizaje supervisado aprendidos en el curso, a través de experimentos reproducibles, y evaluando a su vez la conveniencia de uno u otro, así como la selección de diferentes hiperparámetros a partir del cálculo de las métricas pertinentes.\n",
    "\n",
    "En el caso de nuestro proyecto, nos enfrentamos originalmente a un problema de prediccion ----------------. Sin embargo, a los fines de este práctico, lo transformaremos en un problema de clasificación binario, adaptando las el feature objetivo del dataset. Además, será importante evaluar el desbalance de clases y qué decisiones tomaremos al respecto.\n",
    "\n",
    "Para ello, comenzaremos con las importaciones pertinentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Puede que nos sirvan también\n",
    "import matplotlib as mpl\n",
    "mpl.get_cachedir()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, Ridge\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, classification_report, roc_curve, auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)  # Para mayor determinismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('max_colwidth', 151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigna para Introducción al Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A los fines de realizar este práctico, se utilizará el dataset original.La división entre train y test será realizada en este mismo práctico. A continuación se detallan los pasos a seguir para el preprocesamiento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtención del Dataset\n",
    "\n",
    "Cargar el conjunto de entrenamiento original.\n",
    "\n",
    "2. Aplicar Script de Curación\n",
    "\n",
    "Inicialmente, luego de haber unido ambos datasets, con el objetivo de preparar los datos que alimentarán los modelos de aprendizaje automático (ML) propuestos, deberán aplicar el script de curación obtenido en el práctico anterior. En esta etapa, pueden adicionar los atributos que crean pertinentes a priori o que hayan encontrado interesantes por tener mayor correlación con la variable Target.\n",
    "\n",
    "3. Dataset para Problema de Clasificación Binario\n",
    "\n",
    "Si bien nuestro problema original implica predecir una variable Real, es decir una regresión, comenzaremos por tratarlo como un problema de clasificación binario, en donde nuestro objetivo será:\n",
    "\n",
    "- 1 = Hay distribución de energía (Kw 3 fases > 100)\n",
    "- 0 = No hay distribución de energía  (Kw 3 fases <= 100)\n",
    "\n",
    "Es decir, queremos diferenciar los momentos en que hay cortes en la distribucion de energia de los que no hay. En base a esta definición, deben transformar el dataset para adaptarlo a un problema de clasifiación binario.\n",
    "¿Cómo luce ahora el balance de clases? ¿Tomarán alguna decisión al respecto?\n",
    "\n",
    "4. Normalización de Atributos\n",
    "\n",
    "Es posible que sea necesario normalizar las features de nuestro dataset, dado que muchos de los algoritmos de clasificación supervisada lo requieren. ¿En qué casos tendrá que implementarse normalización?\n",
    "\n",
    "Aplicar a los datasets la normalización de atributos que consideren adecuada.\n",
    "\n",
    "5. Mezcla Aleatória y División en Train/Test\n",
    "\n",
    "Finalmente, están en condiciones de dividir el dataset en Train y Test, utilizando para este último conjunto un 20% de los datos disponibles. Previo a esta división, es recomendable que mezclen los datos aleatoriamente. De este modo, deberán obtener cuatro conjuntos de datos, para cada uno de los datasets: X_train, X_test, y_train y y_test.\n",
    "\n",
    "\n",
    "6. Division en Train/Test (opcional)\n",
    "\n",
    "En muchos de los problemas de series temporales la variable objetivo está muy ligada al momento en la cual se la mide. Es por esto que se suele adoptar diferentes estrategias para la división de los dataset de de entrenamiento y test. \n",
    "\n",
    "La opción más frecuente es dividirlos en subconjuntos ordenados por el tiempo, de manera que el dataset de entrenamiento sea anterior al de test.\n",
    "\n",
    "Prueben realizar esta división y ejecutar los mismos modelos para poder comparar resultados sobre las métricas obtenidas.\n",
    "\n",
    "https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "\n",
    "\n",
    "A modo de ayuda, en esta notebook encontrarán una especie de template que sigue los pasos propuestos y que deberán ir completando.\n",
    "\n",
    "Recuerden que la ciencia de datos es un proceso circular, continuo y no lineal. Es decir, si los datos requieren de mayor procesamiento para satisfacer las necesidades de algoritmos de ML (cualesquiera de ellos), vamos a volver a la etapa inicial para, por ejemplo, crear nuevas features, tomar decisiones diferentes sobre valores faltantes o valores atípicos (outliers), descartar features, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Aplicación de Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez finalizada la etapa de preprocesamiento, se propone implementar diferentes modelos de clasificación para ambos datasets, utilizando la librería Scikit-Learn:\n",
    "\n",
    "    Perceptron. Utilizar el método Stochastic Gradient Descent (Recuerden mezclar aleatoriamente los datos antes de cada iteración)\n",
    "    K Nearest Neighbors ó K Vecinos Más Cercanos\n",
    "    Regresión Logística. Utilizar el método Stochastic Gradient Descent (Recuerden mezclar aleatoriamente los datos antes de cada iteración)\n",
    "\n",
    "Para cada uno de ellos, se pide responder las siguientes consignas:\n",
    "\n",
    "    Utilizar dos features para graficar las clases y la frontera de decisión, siempre que sea posible.\n",
    "    Agregar vector de Bias, cuando lo crean pertinente. Cuándo hace falta y cuándo no? Por qué?\n",
    "    Obtener accuracy o exactitud.\n",
    "\n",
    "De estos tres tipos de modelos, cuál creen que es el más adecuado para nuestro caso de aplicación?\n",
    "\n",
    "Elegir el modelo que consideren que mejor aplica a nuestro problema. Para ello, recuerden que los pasos a seguir en la selección pueden esquematizarse como sigue:\n",
    "1. Descripción de la Hipótesis\n",
    "\n",
    "¿Cuál es nuestro problema? ¿Cómo se caracteriza? ¿Cuál es la hipótesis?\n",
    "2. Selección de Regularizador\n",
    "\n",
    "¿Utilizarán algún regularizador?¿Cuál?\n",
    "3. Selección de Función de Costo\n",
    "\n",
    "¿Cuál será la función de costo utilizada?\n",
    "4. Justificación de las Selecciones\n",
    "\n",
    "¿Por qué eligieron el modelo, el regularizador y la función de costo previas?\n",
    "\n",
    "Finalmente, para el modelo selecionado:\n",
    "\n",
    "    Utilizar el método Grid Search, o de búsqueda exahustiva, con cross-validation para profundizar en la búsqueda y selección de hiperparámetros.\n",
    "    Calcular métricas sobre el conjunto de entrenamiento y de evaluación para los mejores parámetros obtenidos:\n",
    "        Accuracy o exactitud\n",
    "        Reporte de clasificación\n",
    "        Confusion matrix o matriz de confusión (graficar como heatmap)\n",
    "        Curva ROC y área bajo la curva (AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entregables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entregable de este práctico consiste en esta misma Notebook, pero con el preprocesamiento aplicado y los modelos implementados, agregando las explicaciones que crean pertinentes y las decisiones tomadas, en caso de corresponder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
